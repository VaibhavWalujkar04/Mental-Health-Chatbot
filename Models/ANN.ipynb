#ANN with visualisation
import json
import numpy as np
import tensorflow as tf
import pickle
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


with open('pandora_extended_dataset.json') as file:
    data = json.load(file)

texts = []
labels = []
cleaned_intents = []

for intent in data['intents']:
    if 'patterns' in intent and 'responses' in intent and intent['patterns'] and intent['responses']:
        cleaned_intents.append(intent)
        for pattern in intent['patterns']:
            texts.append(pattern)
            labels.append(intent['tag'])

# ------------------- Tokenization -------------------
tokenizer = Tokenizer(num_words=2000, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences, truncating='post', padding='post', maxlen=25)

# ------------------- ðŸ”¹ Label Encoding -------------------
label_encoder = LabelEncoder()
label_encoder.fit(labels)
encoded_labels = label_encoder.transform(labels)
one_hot_labels = tf.keras.utils.to_categorical(encoded_labels)

# ------------------- Build Model -------------------
model = Sequential([
    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=32, input_length=25),
    GlobalAveragePooling1D(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(one_hot_labels.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# ------------------- Train Model with Validation -------------------
history = model.fit(
    padded_sequences,
    one_hot_labels,
    epochs=200,
    validation_split=0,
    verbose=1
)

# ------------------- Save Model and Tokenizers -------------------
model.save("pandora_chatbot_model.h5")
with open("pandora_tokenizer.json", "w") as f:
    f.write(tokenizer.to_json())
with open("pandora_label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

# -------------------  Visualization -------------------
# Accuracy Plot
plt.figure(figsize=(12, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
# plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linestyle='--', linewidth=2)
plt.title('Model Accuracy Over 200 Epochs', fontsize=14)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Loss Plot
plt.figure(figsize=(12, 5))
plt.plot(history.history['loss'], label='Training Loss', linewidth=2)
# plt.plot(history.history['val_loss'], label='Validation Loss', linestyle='--', linewidth=2)
plt.title('Model Loss Over 200 Epochs', fontsize=14)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Perplexity
final_loss = history.history['loss'][-1]
perplexity = np.exp(final_loss)
print(f" Final Loss: {final_loss:.4f}")
print(f" Approximate Perplexity: {perplexity:.2f}")
